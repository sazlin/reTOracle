{
 "metadata": {
  "name": "",
  "signature": "sha256:186863f92a5f03cb8546b131148b3b685c722e4a954e0f687ab0cb0db1b1f0de"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.linear_model import LogisticRegression as LR\n",
      "from string import maketrans\n",
      "import pandas as pd\n",
      "from pandas import DataFrame, read_csv, read_excel, concat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 125
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class ML_builder(object):\n",
      "    def __init__(self):\n",
      "        self.vocab = []\n",
      "        self.classifier = None\n",
      "        self.train_df = None\n",
      "    \n",
      "    def build_vocab(self, n, training_samples = 1000):\n",
      "        vocab = []\n",
      "        with open('stopwords.txt') as stop_words:\n",
      "            stop_words = {line.strip().lower() for line in stop_words if line!='\\n'} \n",
      "        words = {}    \n",
      "        tweets = []\n",
      "        for tweet in self.train_df['SentimentText'][0:training_samples]:\n",
      "            clean_tweet = self.tweet_cleaner(tweet)\n",
      "            tweets.extend(clean_tweet)\n",
      "        for word in tweets:\n",
      "            if word not in stop_words:\n",
      "                if word.isalnum() and len(word)>1:    \n",
      "                    if words.get(word, False):\n",
      "                        words[word] += 1\n",
      "                    else:\n",
      "                        words[word] = 1\n",
      "\n",
      "        vocab = sorted(words.iteritems(), key=lambda x: x[1], reverse=True)\n",
      "        vocab = [tup[0] for tup in vocab]\n",
      "        self.vocab = vocab[:n]\n",
      "        \n",
      "    def tweet_cleaner(self, tweet):\n",
      "        words = []\n",
      "        tweet = tweet.translate(maketrans('?!,.', '    '))\n",
      "        words.extend(tweet.strip().lower().split())\n",
      "        for index, word in enumerate(words):\n",
      "            for happy in happyface:\n",
      "                if happy in word:\n",
      "                    words[index]='happyface'\n",
      "                    break\n",
      "            for sad in sadface:\n",
      "                if sad in word:\n",
      "                    words[index]='sadface'\n",
      "                    break\n",
      "        return words\n",
      "            \n",
      "    def vectorize(self, tweet):\n",
      "        vector = np.zeros(len(self.vocab))\n",
      "        clean_tweet = self.tweet_cleaner(tweet)\n",
      "\n",
      "        for word in clean_tweet:\n",
      "            try:\n",
      "                vector[self.vocab.index(word)] += 1\n",
      "            except ValueError:\n",
      "                pass\n",
      "        return vector\n",
      "\n",
      "    def make_classifier(self, training_samples = 1000):\n",
      "        tweet_array = np.zeros((training_samples, len(self.vocab)))\n",
      "        for index, tweet in enumerate(self.train_df['SentimentText'][0:training_samples]):\n",
      "            tweet_array[index] = self.vectorize(tweet)\n",
      "        output = self.train_df['Sentiment'][0:training_samples]\n",
      "        clf = SVC(kernel='linear', probability=True)\n",
      "        clf.fit(tweet_array, np.asarray(output))\n",
      "        self.classifier = clf\n",
      "\n",
      "    def test_classifier(self, training_samples, test_samples = 400):\n",
      "        test_array = np.zeros((test_samples, len(self.vocab)))\n",
      "        for index, tweet in enumerate(self.train_df['SentimentText'][training_samples:test_samples]):\n",
      "            test_array[index] = self.vectorize(tweet)\n",
      "        predictions = self.classifier.predict(test_array)\n",
      "        correct, wrong = 0,0\n",
      "        for index, predict in enumerate(predictions):\n",
      "            if predict == self.train_df['Sentiment'][index+training_samples]:\n",
      "                correct += 1\n",
      "            else:\n",
      "                wrong += 1\n",
      "        print (correct, wrong)\n",
      "        print 'Percentage {}'.format(1.0*correct/(correct+wrong))\n",
      "\n",
      "    def PCA_analysis(self, training_samples = 1000):\n",
      "        tweet_array = np.zeros((training_samples, len(self.vocab)))\n",
      "        for index, tweet in enumerate(self.train_df['SentimentText'][0:training_samples]):\n",
      "            tweet_array[index] = self.vectorize(tweet)\n",
      "        pca = PCA(n_components=2)\n",
      "        pca.fit(tweet_array)\n",
      "        print(pca.explained_variance_ratio_)\n",
      "        print(pca.components_)\n",
      "        \n",
      "    def ML_build(self):\n",
      "        with open('happyface.txt') as happyface:\n",
      "            happyface = [line.strip() for line in happyface if line!='\\n']\n",
      "            sadface = [line.strip()[::-1] for line in happyface if line!='\\n']\n",
      "\n",
      "        self.train_df = read_csv('Small_sample.csv')\n",
      "        self.train_df = self.train_df.ix[:,['Sentiment','SentimentText']]\n",
      "\n",
      "        self.build_vocab(n = 5000, training_samples = 2500)\n",
      "        self.make_classifier(training_samples = 2500)\n",
      "        self.test_classifier(training_samples = 2500, test_samples = 400)\n",
      "\n",
      "    def Predict(self, tweet):\n",
      "        if self.vocab == []:\n",
      "            self.ML_build()\n",
      "        return (self.classifier.predict(self.vectorize(tweet))[0],\n",
      "                self.classifier.predict_proba(self.vectorize(tweet))[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 253
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TEST.vocab.index('happyface')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 246,
       "text": [
        "118"
       ]
      }
     ],
     "prompt_number": 246
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TEST = ML_builder()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 254
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TEST.ML_build()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(317, 83)\n",
        "Percentage 0.7925\n"
       ]
      }
     ],
     "prompt_number": 255
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hello = TEST.vectorize('I =) PYTHON')\n",
      "hello[118]\n",
      "\n",
      "TEST.Predict('NOTHING')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 275,
       "text": [
        "(0, array([ 0.65261952,  0.34738048]))"
       ]
      }
     ],
     "prompt_number": 275
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}